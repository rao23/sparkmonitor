{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba804dfb",
   "metadata": {},
   "source": [
    "# SparkMonitor VS Code Integration Test\n",
    "\n",
    "This notebook tests the SparkMonitor VS Code extension integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbfd1043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkMonitor VS Code test setup complete\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Import and setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/usr/local/google/home/siddhantrao/new-sparkmonitor/sparkmonitor')\n",
    "\n",
    "print(\"SparkMonitor VS Code test setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b720e40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.sparkmonitor+json": {
       "cellId": "vscode_test_cell",
       "executionCount": 1,
       "jobs": [
        {
         "details": {
          "description": "collect operation on RDD",
          "name": "collect"
         },
         "jobId": 0,
         "msgtype": "sparkJobStart",
         "status": "running",
         "timestamp": 1642781234000
        },
        {
         "details": {
          "numTasks": 4,
          "stageName": "Stage 0: collect"
         },
         "jobId": 0,
         "msgtype": "sparkStageSubmitted",
         "stageId": 0,
         "status": "submitted",
         "timestamp": 1642781234100
        },
        {
         "details": {
          "completedTasks": 4,
          "duration": 2345,
          "numTasks": 4,
          "stageName": "Stage 0: collect"
         },
         "jobId": 0,
         "msgtype": "sparkStageCompleted",
         "stageId": 0,
         "status": "succeeded",
         "timestamp": 1642781236445
        },
        {
         "details": {
          "duration": 2500,
          "name": "collect",
          "result": "Job completed successfully"
         },
         "jobId": 0,
         "msgtype": "sparkJobEnd",
         "status": "succeeded",
         "timestamp": 1642781236734
        }
       ]
      },
      "text/plain": [
       "SparkMonitor: 4 Spark job events"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkMonitor data displayed above!\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Create test SparkMonitor output\n",
    "from IPython.display import display\n",
    "\n",
    "# Sample Spark job data\n",
    "spark_data = {\n",
    "    'cellId': 'vscode_test_cell',\n",
    "    'executionCount': 1,\n",
    "    'jobs': [\n",
    "        {\n",
    "            'msgtype': 'sparkJobStart',\n",
    "            'jobId': 0,\n",
    "            'status': 'running',\n",
    "            'details': {\n",
    "                'name': 'collect',\n",
    "                'description': 'collect operation on RDD'\n",
    "            },\n",
    "            'timestamp': 1642781234000\n",
    "        },\n",
    "        {\n",
    "            'msgtype': 'sparkStageSubmitted',\n",
    "            'jobId': 0,\n",
    "            'stageId': 0,\n",
    "            'status': 'submitted',\n",
    "            'details': {\n",
    "                'stageName': 'Stage 0: collect',\n",
    "                'numTasks': 4\n",
    "            },\n",
    "            'timestamp': 1642781234100\n",
    "        },\n",
    "        {\n",
    "            'msgtype': 'sparkStageCompleted',\n",
    "            'jobId': 0,\n",
    "            'stageId': 0,\n",
    "            'status': 'succeeded',\n",
    "            'details': {\n",
    "                'stageName': 'Stage 0: collect',\n",
    "                'numTasks': 4,\n",
    "                'completedTasks': 4,\n",
    "                'duration': 2345\n",
    "            },\n",
    "            'timestamp': 1642781236445\n",
    "        },\n",
    "        {\n",
    "            'msgtype': 'sparkJobEnd',\n",
    "            'jobId': 0,\n",
    "            'status': 'succeeded',\n",
    "            'details': {\n",
    "                'name': 'collect',\n",
    "                'duration': 2500,\n",
    "                'result': 'Job completed successfully'\n",
    "            },\n",
    "            'timestamp': 1642781236734\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display with the SparkMonitor MIME type\n",
    "display({\n",
    "    'application/vnd.sparkmonitor+json': spark_data,\n",
    "    'text/plain': f\"SparkMonitor: {len(spark_data['jobs'])} Spark job events\"\n",
    "}, raw=True)\n",
    "\n",
    "print(\"✅ SparkMonitor data displayed above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a6e398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.sparkmonitor+json": {
       "cellId": "multi_job_test",
       "executionCount": 2,
       "jobs": [
        {
         "details": {
          "name": "filter + map"
         },
         "jobId": 1,
         "msgtype": "sparkJobStart",
         "status": "running",
         "timestamp": 1753239633333
        },
        {
         "details": {
          "duration": 1200,
          "name": "filter + map"
         },
         "jobId": 1,
         "msgtype": "sparkJobEnd",
         "status": "succeeded",
         "timestamp": 1753239634533
        },
        {
         "details": {
          "name": "reduce"
         },
         "jobId": 2,
         "msgtype": "sparkJobStart",
         "status": "running",
         "timestamp": 1753239634633
        },
        {
         "details": {
          "duration": 800,
          "error": "Out of memory error",
          "name": "reduce"
         },
         "jobId": 2,
         "msgtype": "sparkJobEnd",
         "status": "failed",
         "timestamp": 1753239635433
        }
       ]
      },
      "text/plain": [
       "SparkMonitor: 4 job events (including failed job)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-job test with success and failure cases displayed!\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Multiple jobs simulation\n",
    "import time\n",
    "\n",
    "multi_job_data = {\n",
    "    'cellId': 'multi_job_test',\n",
    "    'executionCount': 2, \n",
    "    'jobs': [\n",
    "        {\n",
    "            'msgtype': 'sparkJobStart',\n",
    "            'jobId': 1,\n",
    "            'status': 'running',\n",
    "            'details': {'name': 'filter + map'},\n",
    "            'timestamp': int(time.time() * 1000)\n",
    "        },\n",
    "        {\n",
    "            'msgtype': 'sparkJobEnd',\n",
    "            'jobId': 1,\n",
    "            'status': 'succeeded',\n",
    "            'details': {'name': 'filter + map', 'duration': 1200},\n",
    "            'timestamp': int(time.time() * 1000) + 1200\n",
    "        },\n",
    "        {\n",
    "            'msgtype': 'sparkJobStart',\n",
    "            'jobId': 2,\n",
    "            'status': 'running', \n",
    "            'details': {'name': 'reduce'},\n",
    "            'timestamp': int(time.time() * 1000) + 1300\n",
    "        },\n",
    "        {\n",
    "            'msgtype': 'sparkJobEnd',\n",
    "            'jobId': 2,\n",
    "            'status': 'failed',\n",
    "            'details': {\n",
    "                'name': 'reduce',\n",
    "                'error': 'Out of memory error',\n",
    "                'duration': 800\n",
    "            },\n",
    "            'timestamp': int(time.time() * 1000) + 2100\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "display({\n",
    "    'application/vnd.sparkmonitor+json': multi_job_data,\n",
    "    'text/plain': f\"SparkMonitor: {len(multi_job_data['jobs'])} job events (including failed job)\"\n",
    "}, raw=True)\n",
    "\n",
    "print(\"✅ Multi-job test with success and failure cases displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795b2fb",
   "metadata": {},
   "source": [
    "## Expected Results\n",
    "\n",
    "When running this notebook in VS Code with the SparkMonitor extension installed, you should see:\n",
    "\n",
    "1. **Rich SparkMonitor displays** above each cell that show job information in a table format\n",
    "2. **Color-coded status indicators** (green for success, red for failure, blue for running)\n",
    "3. **Expandable detail sections** for each job\n",
    "4. **Professional styling** that matches VS Code's theme\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Install the VS Code extension: Copy the `vscode-extension` folder to VS Code extensions\n",
    "2. Enable the SparkMonitor renderer in VS Code\n",
    "3. Integrate with real PySpark jobs by installing the Python kernel extension\n",
    "4. Add React components for advanced visualizations (timeline, task charts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkmonitor-DgSHmWEV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
